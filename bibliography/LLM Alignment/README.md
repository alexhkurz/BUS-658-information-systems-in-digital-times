# LLM Alignment

Research on aligning large language models with human values, preferences, and intentions; RLHF, instruction tuning, safety.

- Aligning to adults is easy, aligning to children is hard: A study of linguistic alignment in dialogue systems - French, D'Mello & von der Wense (2024) - [PDF](https://aclanthology.org/2024.hucllm-1.7.pdf) - [Scholar](https://scholar.google.com/scholar?q=Aligning+to+adults+is+easy+aligning+to+children+is+hard+A+study+of+linguistic+alignment+in+dialogue+systems) - [`french2024`](french2024.md)
- The alignment problem from a deep learning perspective - Ngo, Chan & Mindermann (2024) - [PDF](https://arxiv.org/pdf/2209.00626.pdf) - [Scholar](https://scholar.google.com/scholar?q=The+alignment+problem+from+a+deep+learning+perspective) - [`ngo2024`](ngo2024.md)
- OpenAssistant Conversations - Democratizing large language model alignment - KÃ¶pf et al. (2023) - [PDF](https://proceedings.neurips.cc/paper_files/paper/2023/file/949f0f8f32267d297c2d4e3ee10a2e7e-Paper-Datasets_and_Benchmarks.pdf) - [Scholar](https://scholar.google.com/scholar?q=OpenAssistant+Conversations+Democratizing+Large+Language+Model+Alignment) - [`kopf2023`](kopf2023.md)
- The problem of alignment - Hristova, Magee & Soldatic (2025) - [PDF](https://doi.org/10.1007/s00146-024-02039-2) - [Scholar](https://scholar.google.com/scholar?q=The+problem+of+alignment) - [`hristova2025`](hristova2025.md)
