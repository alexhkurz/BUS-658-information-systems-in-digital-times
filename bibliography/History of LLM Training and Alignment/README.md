# History of LLM Training and Alignment

Historical development of language models, self-supervised learning, word embeddings, transformers, and the evolution towards modern LLMs.

- Prediction and entropy of printed English - Shannon (1951) - [PDF](https://ieeexplore.ieee.org/document/6773263) - [Scholar](https://scholar.google.com/scholar?q=Prediction+and+entropy+of+printed+English) - [`shannon1951`](shannon1951.md)
- "Cloze procedure": A new tool for measuring readability - Taylor (1953) - [PDF](https://doi.org/10.1177/107769905303000401) - [Scholar](https://scholar.google.com/scholar?q=Cloze+procedure+A+new+tool+for+measuring+readability) - [`taylor1953`](taylor1953.md)
- Efficient estimation of word representations in vector space - Mikolov, Chen, Corrado & Dean (2013) - [PDF](https://arxiv.org/pdf/1301.3781.pdf) - [Scholar](https://scholar.google.com/scholar?q=Efficient+Estimation+of+Word+Representations+in+Vector+Space) - [`mikolov2013`](mikolov2013.md)
- Improving language understanding by generative pre-training - Radford, Narasimhan, Salimans & Sutskever (2018) - [PDF](https://cdn.openai.com/research-covers/language-unsupervised/language_understanding_paper.pdf) - [Scholar](https://scholar.google.com/scholar?q=Improving+Language+Understanding+by+Generative+Pre-Training) - [`radford2018`](radford2018.md)
- Chain-of-thought prompting elicits reasoning in large language models - Wei et al. (2022) - [PDF](https://arxiv.org/pdf/2201.11903.pdf) - [Scholar](https://scholar.google.com/scholar?q=Chain-of-Thought+Prompting+Elicits+Reasoning+in+Large+Language+Models) - [`wei2022`](wei2022.md)
- Llama 2: Open foundation and fine-tuned chat models - Touvron et al. (2023) - [PDF](https://arxiv.org/pdf/2307.09288.pdf) - [Scholar](https://scholar.google.com/scholar?q=Llama+2+Open+foundation+and+fine-tuned+chat+models) - [`touvron2023`](touvron2023.md)
