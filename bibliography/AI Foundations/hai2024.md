# Artificial Intelligence Index Report 2024

**Authors:** Stanford HAI (ed.)  
**Year:** 2024  
**Citation key:** `hai2024`  
**PDF:** [Stanford HAI](https://hai.stanford.edu/ai-index/2024)  
**Google Scholar:** [search](https://scholar.google.com/scholar?q=AI+Index+Report+2024+Stanford+HAI)

## Summary

Seventh edition of the AI Index report: technical progress, public perception, and geopolitics of AI. Covers training costs, responsible AI landscape, and a new chapter on AI’s impact on science and medicine. Tracks capabilities, investment, public opinion, and policy. Reports that new LLMs released in 2023 doubled YoY; Gemini Ultra reached human-level on MMLU; GPT-4 scored 0.96 mean win rate on HELM. Aims to support policymakers, researchers, executives, and the public with vetted, broad data on the AI ecosystem.

## Key takeaways

1. **AI beats humans on some tasks, not all.** Surpasses humans on several benchmarks (e.g. image classification, visual reasoning, English understanding); still behind on competition-level math, visual commonsense reasoning, and planning.

2. **Industry dominates frontier AI research.** In 2023, industry produced 51 notable ML models, academia 15; 21 from industry–academia collaborations (a new high).

3. **Frontier models are much more expensive to train.** State-of-the-art training costs are unprecedented: e.g. GPT-4 ~$78M compute, Gemini Ultra ~$191M compute.

4. **The U.S. leads in top AI models.** 61 notable models from U.S.-based institutions in 2023, ahead of the EU (21) and China (15).

5. **Responsible AI evaluations are fragmented.** Lack of standardization in responsible AI reporting; leading developers test against different benchmarks, making systematic comparison of risks and limitations difficult.

6. **Generative AI investment surged.** Overall private AI investment fell for the second year, but funding for generative AI nearly octupled from 2022 to $25.2B (OpenAI, Anthropic, Hugging Face, Inflection, etc.).

7. **AI boosts worker productivity and quality—with caveats.** Studies in 2023 suggest AI helps workers complete tasks faster and improve quality and can narrow the skill gap; others warn that use without proper oversight can reduce performance.

8. **AI is accelerating scientific progress.** 2023 saw major science-related AI applications (e.g. AlphaDev for algorithmic sorting, GNoME for materials discovery).

9. **U.S. AI regulations rose sharply.** 25 AI-related regulations in 2023 (up from 1 in 2016); 56.3% growth in such regulations in 2023 alone.

10. **Public awareness and concern about AI grew.** More people expect AI to dramatically affect their lives in 3–5 years (60% → 66%); 52% report nervousness toward AI (up 13 pp from 2022). In the U.S., 52% more concerned than excited (up from 37% in 2022).

## Other topics covered in the report

- **Patents and geopolitics:** AI patents surged (62.7% growth 2021→2022); **China leads global AI patent origins** (61.1% in 2022 vs U.S. 20.9%); U.S. share has fallen since 2010. Complements takeaway 4 (U.S. leads *models*; China leads *patents*).
- **Open vs closed:** Two-thirds of new LLMs in 2023 were open-source (65.7% vs 44% in 2022), but **closed models significantly outperform open ones** (median +24.2% on 10 benchmarks). Policy and access implications.
- **Deepfakes and elections:** Political deepfakes already affecting elections; synthetic media (e.g. nonconsensual deepfakes) and impact on elections are a major concern for policymakers and the public.
- **Education and talent:** **AI PhD brain drain to industry** (70.7% to industry vs 20% to academia in 2022); less industry-to-academia flow; CS education less international in U.S./Canada; AP CS access unequal (larger/suburban schools); AI degree programs rising globally.
- **Diversity:** U.S./Canada CS graduates growing more ethnically diverse; **gender gaps persist** in European CS/informatics at all levels; U.S. K–12 CS becoming more diverse (gender and ethnicity). Important for pipeline and equity.
- **Policy beyond the U.S.:** **EU AI Act** (landmark deal 2023, enacted 2024); **Biden Executive Order on AI**; AI mentioned in legislative proceedings in **49 countries** (global discourse); more U.S. federal agencies regulating AI (21 in 2023).
- **Multimodal and agentic AI:** Strong multimodal models (Gemini, GPT-4); shift toward **human evaluation** (e.g. Chatbot Arena); **agentic AI** (autonomous agents improving on tasks like Minecraft, shopping, research).
- **Public sentiment nuance:** Only 37% think AI will improve their job; 34% that it will boost the economy. **ChatGPT**: 63% aware globally, ~half of those use at least weekly. Demographic gaps (younger, higher income/education more optimistic).

## Topics missing or underemphasized

*Based on Report Highlights, table of contents, and search of the full report. Not exhaustive.*

**Largely or totally missing**

- **Labor displacement and job destruction.** The report emphasizes productivity gains and “fewer AI job postings”; it does not foreground net job loss, occupational shift by sector, or displacement. Co-directors mention “impacts on employment” but there is no dedicated narrative on displacement.
- **Market concentration and oligopoly.** One mention of “counteract market concentration”; no sustained treatment of concentration in foundation models, cloud providers, data, or talent. Who captures value and who has power is underdeveloped.
- **Military and defense AI.** No chapter or major section on military/defense use, autonomous weapons, or defense R&D. Geopolitics is patents and models, not defense.
- **Global South / Global North divide.** Coverage is heavily U.S., China, EU, UK. Limited treatment of adoption, access, cost, or impact in the Global South; “49 countries” in legislative data does not substitute for distribution of benefits and risks.
- **Long-term and existential safety / alignment.** Responsible AI (evaluation, bias, misuse) is present; existential risk and alignment as policy or research themes do not get a dedicated, prominent place in the highlights.
- **Explainability and interpretability.** No clear standalone treatment in the Report Highlights; “explain its conclusions” appears in the co-directors’ message as a limitation, but interpretability is not a tracked dimension.
- **Surveillance and law enforcement.** Privacy is mentioned; use of AI in surveillance, policing, and border control is not a visible pillar in the highlights.
- **Inequality (who gains).** Beyond diversity in the CS pipeline: distribution of productivity gains, wage effects, and geographic or sectoral inequality get little structured attention.

**Present but underemphasized relative to importance**

- **Environmental impact.** Section 2.13 covers environmental footprint (training, inference, positive use cases) and there are mentions of emissions and power; given the scale of training and inference, carbon and energy could merit more weight and visibility (e.g. in Top 10 or chapter headlines).
- **Misinformation and trust in media.** Cited in the co-directors’ message; not elevated to a Top 10 or chapter-highlight level despite relevance to elections and public discourse.
- **Copyright and creative work.** LLMs outputting copyrighted material appears in Ch 3; broader issues of creative labor, IP, and training on copyrighted works could be expanded and more visible.

**Further gaps**

1. **CapEx vs. revenue / ROI (the bubble question).** The report tracks investment (gen AI funding, GPU spend) and “growth of the industry.” What’s missing: the **return on investment**. There is a large disconnect between hundreds of billions spent on infrastructure (e.g. Nvidia H100s/Blackwell) and the revenue actually generated by software companies using it. For most companies the math does not work yet—a potential economic bubble—but the report does not foreground this.

2. **“Ghost work” supply chain.** The report discusses model performance and benchmarks. What’s missing: the **human cost of the supply chain**. The “intelligence” in AI is often powered by millions of underpaid workers in the Global South (Kenya, Philippines, India) doing psychologically scarring work—labeling hate speech, violence, CSAM—to make models “safe.” The report quantifies model safety, not the harm to the humans who aligned it.

3. **Model collapse and synthetic-data ouroboros.** The report mentions scarcity of high-quality data. What’s missing: **qualitative degradation of the data ecosystem**. As AI fills the web with SEO-spam and synthetic text, future models are increasingly trained on AI-generated content, leading to “model collapse”—models becoming less capable and less diverse over time. The report tracks progress, not this pollution of the training data.

4. **Environmental impact beyond training (water, e-waste).** The report estimates carbon/compute for training. What’s missing: **water footprint** (data centers’ massive use of potable water for cooling, often in drought-prone regions) and **e-waste** (rapid hardware obsolescence—replacing H100s with newer chips every ~18 months—creating a large, rarely quantified e-waste stream).

5. **“Junior developer” crisis (hollowing out the middle).** The report tracks “AI impact on productivity” (e.g. coders 20–50% faster). What’s missing: **destruction of the entry-level apprenticeship**. If AI does juniors’ work, companies hire fewer juniors; if no juniors are hired, who becomes senior in 5 years? The long-term skills gap from over-reliance on copilots is a qualitative/sociological issue that quantitative indexes struggle to capture.

6. **True open source vs. “open weights.”** The report compares closed vs open models (e.g. Llama 3 vs GPT-4). What’s missing: **the definition of “open source” is being narrowed**. Companies like Meta release “open weights” but keep training data and licensing restrictive. Grouping these with true open source obscures **data sovereignty**—real open source implies open data, which is shrinking due to copyright litigation.

7. **Maintaining the open source ecosystem in a corporate-driven economy.** The report tracks who releases “open” vs closed models and where top models originate. What’s missing: **how to sustain the open source ecosystem**—the shared software, tools, and norms on which AI is built—in an economy dominated by corporate incentives to capture value and lock in users. Industry leaders (e.g. **Jensen Huang / Nvidia**) have raised this as a critical concern; Huang has also noted that **China is ahead** in supporting or leveraging open source in this regard, contrasting with the U.S. lead in closed frontier models. The tension between corporate concentration and the commons is a structural gap the index does not frame.

8. **Homogenization of culture (“California-zation” of global thought).** The report tracks performance across languages. What’s missing: **cultural and normative homogenization**. RLHF is largely driven by Western norms and Silicon Valley corporate ethics, so models subtly push a particular worldview onto global users. The index measures translation accuracy, not loss of cultural nuance or the imposition of Western values on non-Western contexts.
