# Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina

**Authors:** Yuan Gao, Dokyun Lee, Gordon Burtch, Sina Fazelpour  
**Year:** 2025  
**Citation key:** `gao2025`  
**PDF:** [arXiv](https://arxiv.org/pdf/2410.19599.pdf)  
**arXiv:** [2410.19599](https://arxiv.org/abs/2410.19599)  
**Google Scholar:** [Search](https://scholar.google.com/scholar?q=Take+Caution+Using+LLMs+Human+Surrogates+Scylla+Ex+Machina+Gao+2025)

## Summary

This paper provides a cautionary assessment of using LLMs as human surrogates in social science research. The authors argue that whilst recent studies suggest LLMs can exhibit human-like reasoning and align with human behavior in economic experiments, surveys, and political discourse, LLMs differ fundamentally from humans—relying on probabilistic patterns without the embodied experiences or survival objectives that shape human cognition. Using the 11-20 money request game, the authors demonstrate that nearly all advanced approaches fail to replicate human behavior distributions across many models. Causes of failure are diverse and unpredictable, relating to input language, roles, and safeguarding. The paper advises caution when using LLMs to study human behavior or as surrogates or simulations, emphasising the need for careful documentation and evaluation.

## Authors

**Yuan Gao** — Questrom School of Business, Information Systems Department, Boston University. Email: yuangg@bu.edu. Research in information systems and AI applications.

**Dokyun Lee** — Questrom School of Business, Information Systems Department and Computing & Data Sciences, Boston University. Email: dokyun@bu.edu. Research in digital platforms, AI, and computational social science.

**Gordon Burtch** — Questrom School of Business, Information Systems Department, Boston University. Email: gburtch@bu.edu. Research in information systems, social computing, and digital platforms.

**Sina Fazelpour** — Department of Philosophy and Khoury College of Computer Sciences, Northeastern University. Email: s.fazel-pour@northeastern.edu. Research in philosophy of AI, ethics, and responsible AI.

## Relevance to our project

### Critical Assessment of LLM Capabilities
- Provides empirical evidence that LLMs fail to replicate human behavior in controlled experiments
- Challenges optimistic claims about LLM surrogacy
- Relevant to understanding limitations and appropriate use cases for LLMs

### Methodological Concerns
- Highlights instability and unpredictability of LLM responses
- Emphasises need for careful documentation and evaluation
- Relevant to discussions of scientific rigor and reproducibility

### Fundamental Differences from Humans
- Argues LLMs lack embodied experiences and survival objectives
- Suggests probabilistic patterns cannot fully capture human cognition
- Relevant to philosophical discussions of what LLMs can and cannot do

### Research Ethics and Responsibility
- Calls for caution in using LLMs as human proxies
- Emphasises importance of understanding limitations
- Relevant to responsible AI research practices

## Key concepts

- **Human surrogates**: Using LLMs to simulate or replace human research subjects
- **11-20 money request game**: Economic game designed to evaluate strategic reasoning depth
- **Behavioral instability**: Unpredictable variation in LLM responses
- **Scylla Ex Machina**: Reference to Homer's Odyssey, suggesting dangerous complexity hidden beneath surface

## Notes

The paper's title references Scylla from Homer's Odyssey—a monster with multiple heads, suggesting that LLMs may appear human-like on the surface but have fundamentally different underlying mechanisms. The 11-20 money request game is particularly interesting because it's designed to test depth of strategic reasoning, and the authors find that LLMs fail systematically across different prompting approaches. The paper emphasises that failures are "diverse and unpredictable," making it difficult to anticipate when LLMs will or won't work as human proxies. This has important implications for social science research considering using LLMs.
