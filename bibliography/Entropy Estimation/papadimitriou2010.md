# Entropy analysis of natural language written texts

**Authors:** C. Papadimitriou, K. Karamanos, F.K. Diakonos, V. Constantoudis, H. Papageorgiou  
**Year:** 2010  
**Citation key:** `papadimitriou2010`  
**PDF:** [ScienceDirect](https://doi.org/10.1016/j.physa.2010.03.038)  
**Google Scholar:** [Search](https://scholar.google.com/scholar?q=Entropy+analysis+of+natural+language+written+texts)

## Summary

The paper investigates the relative contribution of ordered and stochastic components in natural written texts and the influence of text category and language. A binary representation of texts is analysed using block entropy; Shannon and Kolmogorov entropies are computed. Both entropies are sensitive to language and text category, with similar trends across English and Greek. Comparison with stochastically generated sequences helps identify the nature of correlations in this representation of real written texts.

## Authors

**C. Papadimitriou, K. Karamanos, F.K. Diakonos** — Department of Physics, University of Athens, Greece. **V. Constantoudis** — Institute of Microelectronics, NCSR Demokritos, Athens. **H. Papageorgiou** — Institute for Language and Speech Processing, Athens. Research in statistical physics, symbolic dynamics, and quantitative linguistics.

## Relevance to our class

- **Entropy of language**: Shannon and Kolmogorov entropy as tools for written text.
- **Cross-linguistic and register variation**: sensitivity to language and text category.
- **Connections** between statistical physics (critical phenomena, scaling) and language structure.

## Key concepts

- **Block entropy**, Shannon entropy, Kolmogorov entropy, symbolic sequences, text category, lexical/statistical structure.

## Notes

(Quotes, reading notes, follow-ups)
