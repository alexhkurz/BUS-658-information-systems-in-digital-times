# Why One Cannot Estimate the Entropy of English by Sampling

**Authors:** Joachim von zur Gathen, Daniel Loebenberger  
**Year:** 2018  
**Citation key:** `vonzurgathen2018`  
**PDF:** [Taylor & Francis](https://doi.org/10.1080/09296174.2017.1341724)  
**Google Scholar:** [Search](https://scholar.google.com/scholar?q=Why+One+Cannot+Estimate+the+Entropy+of+English+by+Sampling)

## Summary

The authors aimed to refine estimates of the entropy of English using large corpora (e.g. COCA) but found that frequency counting in corpora does not yield reliable entropy estimates. Using a simplified language model related to those in the literature, they show a trichotomy: for very small n (e.g. n ≤ 4), n-gram counting is reasonably reliable; for medium n (up to ~14), statistical noise grows; beyond that, estimates are dominated by noise. The model gives explicit thresholds depending on corpus size. The work supports the linguistic intuition that frequency counting in (large) corpora is insufficient for approximating the entropy of English and that other linguistic and mathematical tools are needed.

## Authors

**Joachim von zur Gathen, Daniel Loebenberger** — Bonn–Aachen International Center for Information Technology, Universität Bonn, Germany. Research in algorithms, cryptography, and quantitative linguistics. von zur Gathen is known for computational algebra and cryptography; the entropy-of-English problem is motivated in part by Shannon’s unicity bound in cryptanalysis.

## Relevance to our class

- **Limits of corpus-based entropy estimation**; connection to Shannon’s 1951 work on the entropy of English.
- **Cryptography**: entropy of plaintext and unicity bound.
- **Methodology**: why “more data” does not solve the estimation problem without a proper model.

## Key concepts

- **Entropy of English**, n-gram counting, corpus size, statistical noise, trichotomy, unicity bound, language model.

## Notes

(Quotes, reading notes, follow-ups)
