# The Word Entropy of Natural Languages

**Authors:** Christian Bentz, Dimitrios Alikaniotis  
**Year:** 2016  
**Citation key:** `bentz2016`  
**PDF:** [arXiv](https://arxiv.org/pdf/1606.06996.pdf)  
**arXiv:** [1606.06996](https://arxiv.org/abs/1606.06996)  
**Google Scholar:** [Search](https://scholar.google.com/scholar?q=The+Word+Entropy+of+Natural+Languages)

## Summary

Word entropy (average uncertainty associated with words) is central to quantitative and computational linguistics. Using parallel texts in 21 languages, the authors determine the token counts at which word entropies converge to stable values, then use a massively parallel corpus to estimate word entropies across more than 1000 languages. Results support quantitative language comparison, understanding of multilingual translation performance, and normalisation of semantic similarity measures.

## Authors

**Christian Bentz** — Department of Linguistics, University of Tübingen, Germany. **Dimitrios Alikaniotis** — Department of Theoretical and Applied Linguistics, University of Cambridge, UK. Research in quantitative linguistics, entropy, and multilingual NLP.

## Relevance to our class

- **Entropy as a linguistic measure**: convergence and cross-linguistic variation.
- **Data requirements**: minimum text size for reliable entropy estimation.
- **Applications**: machine translation, distributional semantics, information retrieval.

## Key concepts

- **Word entropy**, block entropy, source entropy estimator, convergence, parallel corpora, multilingual comparison.

## Notes

(Quotes, reading notes, follow-ups)
