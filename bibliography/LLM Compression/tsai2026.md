# Revisiting Data Compression with Language Modeling

**Authors:** Chen-Han Tsai  
**Year:** 2026  
**Citation key:** `tsai2026`  
**PDF:** [arXiv](https://arxiv.org/pdf/2601.02875.pdf)  
**arXiv:** [2601.02875](https://arxiv.org/abs/2601.02875)  
**Google Scholar:** [Search](https://scholar.google.com/scholar?q=Revisiting+Data+Compression+with+Language+Modeling)

## Summary

This report revisits the use of large language models for data compression. It builds on prior work that applied LLMs to text and multi-modal data and focuses on reducing the adjusted compression rate and improving performance on different encodings (including byte-stream). The author shows that the memory footprint of existing LLMs can be reduced while keeping similar compression levels, achieving a new state-of-the-art adjusted compression rate (e.g. nearly 18%) without extra training. LLMs are shown to be strong compressors for text-dominant data and to remain competitive on non-text data when configured appropriately.

## Authors

**Chen-Han Tsai** â€” Independent researcher. Contact: maxwelltsai@yahoo.com. Focus on LLM-based compression and efficiency.

## Relevance to our class

- **Efficiency vs capability**: compressing the compressor (smaller LMs) while preserving compression performance.
- **Generalisation**: LLMs as compressors across text and non-text data.
- **Practical deployment**: adjusted compression rate and memory footprint as design criteria.

## Key concepts

- **Adjusted compression rate**, LLM compression, byte-stream encoding, memory footprint, state-of-the-art compression.

## Notes

(Quotes, reading notes, follow-ups)
