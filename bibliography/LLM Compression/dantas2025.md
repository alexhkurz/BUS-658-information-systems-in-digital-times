# A review of state-of-the-art techniques for large language model compression

**Authors:** Pierre V. Dantas, Lucas C. Cordeiro, Waldir S. S. Junior  
**Year:** 2025  
**Citation key:** `dantas2025`  
**PDF:** [Springer](https://doi.org/10.1007/s40747-025-02019-z)  
**Google Scholar:** [Search](https://scholar.google.com/scholar?q=A+review+of+state-of-the-art+techniques+for+large+language+model+compression)

## Summary

Survey of state-of-the-art LLM compression techniques: pruning, quantization, knowledge distillation, and neural architecture search (NAS). It presents an evaluation framework combining accuracy/perplexity with latency–accuracy trade-offs, parameter efficiency, multi-objective Pareto optimisation, and fairness. Topics include fairness-aware compression, robustness to adversarial attacks, and hardware-specific optimisations. NAS-driven and hybrid/adaptive methods for task- and hardware-aware efficient architectures are discussed. The paper synthesises recent advances, open problems, and a research roadmap for efficient, scalable, and equitable LLMs in real-world deployment.

## Authors

**Pierre V. Dantas** — Department of Computer Science, University of Manchester, UK. **Lucas C. Cordeiro** — University of Manchester. **Waldir S. S. Junior** — Department of Electrical Engineering, Federal University of Amazonas (UFAM), Manaus, Brazil. Research in software verification, compilers, and efficient ML systems.

## Relevance to our class

- **Model compression** as a central theme for deployment (edge, mobile, cloud).
- **Trade-offs**: size, speed, accuracy, fairness, robustness.
- **Taxonomy** of methods (pruning, quantization, distillation, NAS) and evaluation criteria.

## Key concepts

- **Pruning**, quantization, knowledge distillation, NAS, latency–accuracy trade-off, Pareto optimisation, fairness-aware compression, adversarial robustness, edge computing, adaptive compression.

## Notes

(Quotes, reading notes, follow-ups)
