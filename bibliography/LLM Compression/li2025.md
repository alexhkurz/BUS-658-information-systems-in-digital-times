# Lossless data compression by large models

**Authors:** Ziguang Li, Chao Huang, Xuliang Wang, Haibo Hu, Cole Wyeth, Dongbo Bu, Quan Yu, Wen Gao, Xingwu Liu, Ming Li  
**Year:** 2025  
**Citation key:** `li2025`  
**PDF:** [Nature Machine Intelligence](https://doi.org/10.1038/s42256-025-01033-7)  
**Google Scholar:** [Search](https://scholar.google.com/scholar?q=Lossless+data+compression+by+large+models)

## Summary

LMCompress is a method that uses large models to perform lossless compression. It improves on previous lossless compression records across four media types: text, images, video, and audio—halving compression rates versus JPEG-XL (images), FLAC (audio), and H.264 (video), and reaching about one-third of zpaq’s rate for text. The paper argues that semantics convey meaning concisely and that large models can revolutionise compression by “understanding” data. It uses autoregressive generative models (including image GPT for images/video and bGPT-audio for audio) with arithmetic coding, and positions the work as a broad validation that better understanding implies better compression.

## Authors

**Ziguang Li, Chao Huang, Xuliang Wang, Haibo Hu** (equal contribution) — Central China Institute of Artificial Intelligence; Institute of Computing Technology, Chinese Academy of Sciences; Ningbo Institute of AI Industry; University of Waterloo; Peng Cheng Laboratory; Shanghai Institute for Mathematics and Interdisciplinary Sciences; Dalian University of Technology. **Ming Li** — University of Waterloo, Peng Cheng Laboratory. Research in compression, algorithmic information theory, and large models.

## Relevance to our class

- **Understanding and compression**: “better understanding implies better compression” as a unifying principle.
- **Multi-modal compression**: same conceptual framework for text, image, video, audio.
- **Theoretical roots**: Solomonoff induction and the limits of traditional (e.g. Shannon-style) compression.

## Key concepts

- **LMCompress**, lossless compression, arithmetic coding, autoregressive models, image GPT, bGPT-audio, multi-modal compression, Solomonoff induction.

## Notes

(Quotes, reading notes, follow-ups)
