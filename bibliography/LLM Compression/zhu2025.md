# A Survey on Model Compression for Large Language Models

**Authors:** Xunyu Zhu, Jian Li, Yong Liu, Can Ma, Weiping Wang  
**Year:** 2025  
**Citation key:** `zhu2025`  
**PDF:** [TACL](https://aclanthology.org/)  
**Google Scholar:** [Search](https://scholar.google.com/scholar?q=A+Survey+on+Model+Compression+for+Large+Language+Models)

## Summary

Survey of model compression for LLMs. It covers quantization, pruning, knowledge distillation, and low-rank factorisation, with recent advances, benchmarking strategies, and evaluation metrics for compressed LLMs. The goal is to improve efficiency and real-world applicability while preserving performance, and to provide a foundation for future work. Authors are from Chinese Academy of Sciences, Renmin University of China, and Beijing Normal University.

## Authors

**Xunyu Zhu, Can Ma, Weiping Wang** — Institute of Information Engineering, Chinese Academy of Sciences; School of Cyber Security, University of Chinese Academy of Sciences. **Yong Liu** — Gaoling School of Artificial Intelligence, Renmin University of China. **Jian Li** — School of Artificial Intelligence, Beijing Normal University. Research in NLP, model compression, and efficient LLM deployment.

## Relevance to our class

- **Taxonomy** of LLM compression (quantization, pruning, distillation, low-rank).
- **Metrics and benchmarks** for evaluating compressed models.
- **Deployment** on resource-constrained devices and inference efficiency.

## Key concepts

- **Model compression**, quantization, pruning, knowledge distillation, low-rank factorisation, benchmarking, inference efficiency.

## Notes

(Quotes, reading notes, follow-ups)
