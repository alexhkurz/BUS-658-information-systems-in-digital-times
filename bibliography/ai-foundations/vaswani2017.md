# Attention Is All You Need

**Authors:** Ashish Vaswani, Llion Jones, Noam Shazeer, Niki Parmar, Aidan N. Gomez, Jakob Uszkoreit, Łukasz Kaiser, Illia Polosukhin  
**Year:** 2017  
**Citation key:** `vaswani2017`  
**PDF:** [arXiv](https://arxiv.org/pdf/1706.03762.pdf)  
**arXiv:** [1706.03762](https://arxiv.org/abs/1706.03762)  
**Google Scholar:** [search](https://scholar.google.com/scholar?q=Attention+Is+All+You+Need+Vaswani+2017)

## Summary

The paper introduces the **Transformer** architecture, which relies entirely on self-attention (no recurrence or convolution). It achieves state-of-the-art on WMT 2014 English–German and English–French machine translation and generalizes to English constituency parsing. Scaled dot-product attention and multi-head attention are the core mechanisms; position is encoded via fixed sinusoids. The model is more parallelizable and trains faster than prior encoder–decoder RNN/CNN models.

## Key concepts

- Transformer, self-attention, scaled dot-product attention, multi-head attention
- Encoder–decoder without RNN/CNN
- Positional encoding
- WMT translation, BLEU

## Notes

(To be filled on closer reading.)
